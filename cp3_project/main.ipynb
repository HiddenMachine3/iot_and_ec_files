{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TPSA(Tot)</th>\n",
       "      <th>SAacc</th>\n",
       "      <th>H-050</th>\n",
       "      <th>MLOGP</th>\n",
       "      <th>RDCHI</th>\n",
       "      <th>GATS1p</th>\n",
       "      <th>nN</th>\n",
       "      <th>C-040</th>\n",
       "      <th>quantitative response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.419</td>\n",
       "      <td>1.225</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.638</td>\n",
       "      <td>1.401</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.23</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.799</td>\n",
       "      <td>2.930</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.23</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.453</td>\n",
       "      <td>2.887</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.23</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.068</td>\n",
       "      <td>2.758</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TPSA(Tot)  SAacc  H-050  MLOGP  RDCHI  GATS1p  nN  C-040  \\\n",
       "0       0.00    0.0      0  2.419  1.225   0.667   0      0   \n",
       "1       0.00    0.0      0  2.638  1.401   0.632   0      0   \n",
       "2       9.23   11.0      0  5.799  2.930   0.486   0      0   \n",
       "3       9.23   11.0      0  5.453  2.887   0.495   0      0   \n",
       "4       9.23   11.0      0  4.068  2.758   0.695   0      0   \n",
       "\n",
       "   quantitative response  \n",
       "0                  3.740  \n",
       "1                  4.330  \n",
       "2                  7.019  \n",
       "3                  6.723  \n",
       "4                  5.979  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [\"TPSA(Tot)\", \"SAacc\", \"H-050\", \"MLOGP\", \"RDCHI\", \"GATS1p\", \"nN\", \"C-040\", \"quantitative response\"]\n",
    "df = pd.read_csv('qsar_aquatic_toxicity.csv', names=column_names, delimiter=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "x_train, y_train = train_df.drop('quantitative response', axis=1), train_df['quantitative response']\n",
    "x_test, y_test = test_df.drop('quantitative response', axis=1), test_df['quantitative response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TPSA(Tot)</th>\n",
       "      <th>SAacc</th>\n",
       "      <th>H-050</th>\n",
       "      <th>MLOGP</th>\n",
       "      <th>RDCHI</th>\n",
       "      <th>GATS1p</th>\n",
       "      <th>nN</th>\n",
       "      <th>C-040</th>\n",
       "      <th>quantitative response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>4.760</td>\n",
       "      <td>2.868</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.291</td>\n",
       "      <td>1.549</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>9.23</td>\n",
       "      <td>11.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800</td>\n",
       "      <td>1.509</td>\n",
       "      <td>2.003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>20.23</td>\n",
       "      <td>42.683</td>\n",
       "      <td>1</td>\n",
       "      <td>2.461</td>\n",
       "      <td>1.975</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>9.23</td>\n",
       "      <td>11.000</td>\n",
       "      <td>0</td>\n",
       "      <td>3.275</td>\n",
       "      <td>2.727</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TPSA(Tot)   SAacc  H-050  MLOGP  RDCHI  GATS1p  nN  C-040  \\\n",
       "421       0.00   0.000      0  4.760  2.868   0.700   0      0   \n",
       "214       0.00   0.000      0  3.291  1.549   2.000   0      0   \n",
       "106       9.23  11.000      0  0.800  1.509   2.003   0      0   \n",
       "253      20.23  42.683      1  2.461  1.975   0.737   0      0   \n",
       "542       9.23  11.000      0  3.275  2.727   0.874   0      0   \n",
       "\n",
       "     quantitative response  \n",
       "421                  6.281  \n",
       "214                  4.927  \n",
       "106                  1.730  \n",
       "253                  5.691  \n",
       "542                  3.953  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               1152      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,281\n",
      "Trainable params: 1,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DENSE1_SIZE = 128\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Input(shape=(x_train.shape[1:])),\n",
    "  tf.keras.layers.Dense(DENSE1_SIZE, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(x_test.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.28595"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(y_test.to_numpy(), predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 1s 54ms/step - loss: 89.5094 - mse: 89.5094 - mae: 6.3797 - val_loss: 15.8821 - val_mse: 15.8821 - val_mae: 3.2914\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 47.1093 - mse: 47.1093 - mae: 5.2164 - val_loss: 11.8617 - val_mse: 11.8617 - val_mae: 2.7674\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 48.9977 - mse: 48.9977 - mae: 4.8634 - val_loss: 8.5990 - val_mse: 8.5990 - val_mae: 2.4188\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 43.4456 - mse: 43.4456 - mae: 4.6459 - val_loss: 7.0749 - val_mse: 7.0749 - val_mae: 2.1809\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 51.1610 - mse: 51.1610 - mae: 4.5274 - val_loss: 5.5961 - val_mse: 5.5961 - val_mae: 1.9490\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 40.4675 - mse: 40.4675 - mae: 4.2113 - val_loss: 4.1575 - val_mse: 4.1575 - val_mae: 1.5575\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 41.9097 - mse: 41.9097 - mae: 3.7213 - val_loss: 4.4120 - val_mse: 4.4120 - val_mae: 1.6443\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 38.0063 - mse: 38.0063 - mae: 3.6777 - val_loss: 11.0313 - val_mse: 11.0313 - val_mae: 2.5896\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 32.9138 - mse: 32.9138 - mae: 3.5723 - val_loss: 11.2212 - val_mse: 11.2212 - val_mae: 2.2839\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 34.0508 - mse: 34.0508 - mae: 3.6092 - val_loss: 4.7223 - val_mse: 4.7223 - val_mae: 1.6491\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 31.9284 - mse: 31.9284 - mae: 3.2429 - val_loss: 9.1188 - val_mse: 9.1188 - val_mae: 2.0428\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 24.9746 - mse: 24.9746 - mae: 3.2279 - val_loss: 3.3276 - val_mse: 3.3276 - val_mae: 1.2927\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 29.4303 - mse: 29.4303 - mae: 3.2176 - val_loss: 5.6910 - val_mse: 5.6910 - val_mae: 1.7107\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 24.3166 - mse: 24.3166 - mae: 2.9600 - val_loss: 3.0366 - val_mse: 3.0366 - val_mae: 1.2707\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 21.0083 - mse: 21.0083 - mae: 2.7801 - val_loss: 2.6250 - val_mse: 2.6250 - val_mae: 1.1880\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 29.6102 - mse: 29.6102 - mae: 2.9989 - val_loss: 13.2295 - val_mse: 13.2295 - val_mae: 2.7204\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 23.5425 - mse: 23.5425 - mae: 2.8243 - val_loss: 3.9496 - val_mse: 3.9496 - val_mae: 1.5373\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 19.2345 - mse: 19.2345 - mae: 2.6623 - val_loss: 4.9368 - val_mse: 4.9368 - val_mae: 1.6198\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 16.3868 - mse: 16.3868 - mae: 2.5022 - val_loss: 2.4474 - val_mse: 2.4474 - val_mae: 1.2323\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 13.3539 - mse: 13.3539 - mae: 2.4914 - val_loss: 2.2709 - val_mse: 2.2709 - val_mae: 1.1653\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 16.4468 - mse: 16.4468 - mae: 2.5508 - val_loss: 2.8289 - val_mse: 2.8289 - val_mae: 1.3304\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 17.9728 - mse: 17.9728 - mae: 2.3848 - val_loss: 2.6484 - val_mse: 2.6484 - val_mae: 1.1863\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 21.0604 - mse: 21.0604 - mae: 2.5712 - val_loss: 3.1540 - val_mse: 3.1540 - val_mae: 1.3594\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 17.0291 - mse: 17.0291 - mae: 2.6284 - val_loss: 5.0336 - val_mse: 5.0336 - val_mae: 1.5659\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 14.7278 - mse: 14.7278 - mae: 2.4434 - val_loss: 5.0733 - val_mse: 5.0733 - val_mae: 1.6955\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 12.4595 - mse: 12.4595 - mae: 2.3023 - val_loss: 4.2572 - val_mse: 4.2572 - val_mae: 1.5075\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 16.9191 - mse: 16.9191 - mae: 2.3624 - val_loss: 2.2695 - val_mse: 2.2695 - val_mae: 1.1221\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 11.7195 - mse: 11.7195 - mae: 2.2601 - val_loss: 4.3301 - val_mse: 4.3301 - val_mae: 1.4889\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 11.6992 - mse: 11.6992 - mae: 2.1775 - val_loss: 2.5980 - val_mse: 2.5980 - val_mae: 1.1650\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.6126 - mse: 9.6126 - mae: 2.0841 - val_loss: 2.2838 - val_mse: 2.2838 - val_mae: 1.1063\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.1702 - mse: 9.1702 - mae: 2.0263 - val_loss: 2.2000 - val_mse: 2.2000 - val_mae: 1.1463\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.8170 - mse: 9.8170 - mae: 2.0472 - val_loss: 2.2777 - val_mse: 2.2777 - val_mae: 1.0911\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 10.9028 - mse: 10.9028 - mae: 2.0065 - val_loss: 2.4089 - val_mse: 2.4089 - val_mae: 1.2216\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 9.6651 - mse: 9.6651 - mae: 2.0344 - val_loss: 3.2835 - val_mse: 3.2835 - val_mae: 1.3862\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 8.5660 - mse: 8.5660 - mae: 1.9214 - val_loss: 2.2657 - val_mse: 2.2657 - val_mae: 1.0862\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 11.8617 - mse: 11.8617 - mae: 2.0692 - val_loss: 5.9500 - val_mse: 5.9500 - val_mae: 1.7171\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 8.8438 - mse: 8.8438 - mae: 1.9093 - val_loss: 2.4359 - val_mse: 2.4359 - val_mae: 1.1261\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 7.0833 - mse: 7.0833 - mae: 1.7418 - val_loss: 2.5159 - val_mse: 2.5159 - val_mae: 1.2231\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 7.2114 - mse: 7.2114 - mae: 1.7533 - val_loss: 2.2881 - val_mse: 2.2881 - val_mae: 1.1562\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 9.4565 - mse: 9.4565 - mae: 1.8256 - val_loss: 2.6206 - val_mse: 2.6206 - val_mae: 1.2507\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 5.3567 - mse: 5.3567 - mae: 1.6884 - val_loss: 2.4479 - val_mse: 2.4479 - val_mae: 1.1408\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 7.0799 - mse: 7.0799 - mae: 1.7453 - val_loss: 2.1295 - val_mse: 2.1295 - val_mae: 1.1287\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 6.9185 - mse: 6.9185 - mae: 1.7103 - val_loss: 2.0830 - val_mse: 2.0830 - val_mae: 1.0693\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.1485 - mse: 9.1485 - mae: 1.8333 - val_loss: 2.1690 - val_mse: 2.1690 - val_mae: 1.0831\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.9046 - mse: 5.9046 - mae: 1.6434 - val_loss: 2.2518 - val_mse: 2.2518 - val_mae: 1.1621\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.8352 - mse: 5.8352 - mae: 1.6035 - val_loss: 2.1269 - val_mse: 2.1269 - val_mae: 1.0675\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4.9619 - mse: 4.9619 - mae: 1.5890 - val_loss: 2.0690 - val_mse: 2.0690 - val_mae: 1.0869\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 6.3638 - mse: 6.3638 - mae: 1.6414 - val_loss: 2.4639 - val_mse: 2.4639 - val_mae: 1.1504\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.6950 - mse: 5.6950 - mae: 1.6317 - val_loss: 2.1753 - val_mse: 2.1753 - val_mae: 1.0969\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 6.0163 - mse: 6.0163 - mae: 1.5865 - val_loss: 2.0348 - val_mse: 2.0348 - val_mae: 1.0783\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.3796 - mse: 5.3796 - mae: 1.6103 - val_loss: 2.0692 - val_mse: 2.0692 - val_mae: 1.0642\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.6467 - mse: 5.6467 - mae: 1.6241 - val_loss: 2.1083 - val_mse: 2.1083 - val_mae: 1.1122\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.9965 - mse: 3.9965 - mae: 1.4480 - val_loss: 2.1067 - val_mse: 2.1067 - val_mae: 1.1284\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.6342 - mse: 5.6342 - mae: 1.6243 - val_loss: 1.9835 - val_mse: 1.9835 - val_mae: 1.0492\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4.1936 - mse: 4.1936 - mae: 1.4383 - val_loss: 2.1720 - val_mse: 2.1720 - val_mae: 1.1405\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 6.4474 - mse: 6.4474 - mae: 1.5630 - val_loss: 2.0492 - val_mse: 2.0492 - val_mae: 1.0431\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.9992 - mse: 3.9992 - mae: 1.4188 - val_loss: 2.0506 - val_mse: 2.0506 - val_mae: 1.0684\n",
      "Epoch 58/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.5454 - mse: 3.5454 - mae: 1.3468 - val_loss: 2.0812 - val_mse: 2.0812 - val_mae: 1.0609\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.2731 - mse: 4.2731 - mae: 1.4233 - val_loss: 2.2045 - val_mse: 2.2045 - val_mae: 1.1551\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4.0287 - mse: 4.0287 - mae: 1.4529 - val_loss: 2.0240 - val_mse: 2.0240 - val_mae: 1.0841\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.5982 - mse: 3.5982 - mae: 1.3252 - val_loss: 2.1015 - val_mse: 2.1015 - val_mae: 1.0529\n",
      "Epoch 62/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.4694 - mse: 4.4694 - mae: 1.4681 - val_loss: 2.3023 - val_mse: 2.3023 - val_mae: 1.1882\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.5150 - mse: 4.5150 - mae: 1.4233 - val_loss: 2.3242 - val_mse: 2.3242 - val_mae: 1.1250\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.8335 - mse: 3.8335 - mae: 1.3377 - val_loss: 2.0670 - val_mse: 2.0670 - val_mae: 1.1033\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.6453 - mse: 3.6453 - mae: 1.3529 - val_loss: 2.2255 - val_mse: 2.2255 - val_mae: 1.0803\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.1932 - mse: 5.1932 - mae: 1.5280 - val_loss: 2.4714 - val_mse: 2.4714 - val_mae: 1.1978\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.0825 - mse: 3.0825 - mae: 1.2857 - val_loss: 2.2258 - val_mse: 2.2258 - val_mae: 1.0861\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6697 - mse: 3.6697 - mae: 1.3854 - val_loss: 2.4155 - val_mse: 2.4155 - val_mae: 1.2183\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.9981 - mse: 2.9981 - mae: 1.2578 - val_loss: 2.0751 - val_mse: 2.0751 - val_mae: 1.0496\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.3820 - mse: 3.3820 - mae: 1.3470 - val_loss: 1.9847 - val_mse: 1.9847 - val_mae: 1.0400\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3.0531 - mse: 3.0531 - mae: 1.3170 - val_loss: 1.9713 - val_mse: 1.9713 - val_mae: 1.0287\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3.0623 - mse: 3.0623 - mae: 1.2500 - val_loss: 1.9841 - val_mse: 1.9841 - val_mae: 1.0406\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3.3100 - mse: 3.3100 - mae: 1.2659 - val_loss: 2.1563 - val_mse: 2.1563 - val_mae: 1.1248\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3.0220 - mse: 3.0220 - mae: 1.2510 - val_loss: 2.0478 - val_mse: 2.0478 - val_mae: 1.0671\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3.1830 - mse: 3.1830 - mae: 1.2674 - val_loss: 2.0189 - val_mse: 2.0189 - val_mae: 1.0034\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 3.4783 - mse: 3.4783 - mae: 1.2460 - val_loss: 2.2383 - val_mse: 2.2383 - val_mae: 1.1046\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 2.7674 - mse: 2.7674 - mae: 1.1900 - val_loss: 2.0807 - val_mse: 2.0807 - val_mae: 1.1083\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2.8112 - mse: 2.8112 - mae: 1.2374 - val_loss: 2.0292 - val_mse: 2.0292 - val_mae: 1.0642\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.9680 - mse: 2.9680 - mae: 1.2601 - val_loss: 2.0575 - val_mse: 2.0575 - val_mae: 1.1023\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.8219 - mse: 2.8219 - mae: 1.2457 - val_loss: 1.9613 - val_mse: 1.9613 - val_mae: 1.0291\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 2.8128 - mse: 2.8128 - mae: 1.2020 - val_loss: 2.0135 - val_mse: 2.0135 - val_mae: 1.0699\n",
      "Epoch 82/100\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 2.7693 - mse: 2.7693 - mae: 1.1749 - val_loss: 2.1113 - val_mse: 2.1113 - val_mae: 1.0436\n",
      "Epoch 83/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 2.8275 - mse: 2.8275 - mae: 1.1795 - val_loss: 1.9916 - val_mse: 1.9916 - val_mae: 1.0722\n",
      "Epoch 84/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.0986 - mse: 3.0986 - mae: 1.2386 - val_loss: 1.9468 - val_mse: 1.9468 - val_mae: 1.0177\n",
      "Epoch 85/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2.7768 - mse: 2.7768 - mae: 1.1955 - val_loss: 2.1270 - val_mse: 2.1270 - val_mae: 1.0584\n",
      "Epoch 86/100\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 2.4872 - mse: 2.4872 - mae: 1.1703 - val_loss: 1.9978 - val_mse: 1.9978 - val_mae: 1.0649\n",
      "Epoch 87/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.4657 - mse: 2.4657 - mae: 1.1134 - val_loss: 1.9839 - val_mse: 1.9839 - val_mae: 1.0637\n",
      "Epoch 88/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.4976 - mse: 2.4976 - mae: 1.1849 - val_loss: 2.0286 - val_mse: 2.0286 - val_mae: 1.0810\n",
      "Epoch 89/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 3.1185 - mse: 3.1185 - mae: 1.2338 - val_loss: 2.1437 - val_mse: 2.1437 - val_mae: 1.0505\n",
      "Epoch 90/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 3.0687 - mse: 3.0687 - mae: 1.2089 - val_loss: 2.2150 - val_mse: 2.2150 - val_mae: 1.1613\n",
      "Epoch 91/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.2917 - mse: 2.2917 - mae: 1.1294 - val_loss: 1.9667 - val_mse: 1.9667 - val_mae: 1.0367\n",
      "Epoch 92/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.7577 - mse: 2.7577 - mae: 1.2014 - val_loss: 2.1089 - val_mse: 2.1089 - val_mae: 1.1031\n",
      "Epoch 93/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.9217 - mse: 2.9217 - mae: 1.2391 - val_loss: 2.1123 - val_mse: 2.1123 - val_mae: 1.0522\n",
      "Epoch 94/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.3349 - mse: 2.3349 - mae: 1.1519 - val_loss: 2.0166 - val_mse: 2.0166 - val_mae: 1.0706\n",
      "Epoch 95/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.5579 - mse: 2.5579 - mae: 1.2136 - val_loss: 1.9813 - val_mse: 1.9813 - val_mae: 1.0354\n",
      "Epoch 96/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.2356 - mse: 2.2356 - mae: 1.0723 - val_loss: 1.9748 - val_mse: 1.9748 - val_mae: 1.0340\n",
      "Epoch 97/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.6374 - mse: 2.6374 - mae: 1.1931 - val_loss: 2.0831 - val_mse: 2.0831 - val_mae: 1.0952\n",
      "Epoch 98/100\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.4564 - mse: 2.4564 - mae: 1.1387 - val_loss: 2.2251 - val_mse: 2.2251 - val_mae: 1.0714\n",
      "Epoch 99/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.0041 - mse: 2.0041 - mae: 1.0750 - val_loss: 2.0109 - val_mse: 2.0109 - val_mae: 1.0581\n",
      "Epoch 100/100\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.2859 - mse: 2.2859 - mae: 1.1439 - val_loss: 1.9665 - val_mse: 1.9665 - val_mae: 1.0369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b2e247ef48>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_OF_EPOCHS = 100\n",
    "model.fit(x_train, y_train,validation_data=(x_test,y_test), epochs=NUM_OF_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 0s - loss: 1.9665 - mse: 1.9665 - mae: 1.0369 - 61ms/epoch - 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.966518521308899, 1.966518521308899, 1.0369356870651245]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.6626976],\n",
       "       [ 6.380378 ],\n",
       "       [ 3.4101114],\n",
       "       [ 5.137807 ],\n",
       "       [ 4.39334  ],\n",
       "       [ 5.1015463],\n",
       "       [ 4.887074 ],\n",
       "       [ 5.345476 ],\n",
       "       [ 4.672554 ],\n",
       "       [ 5.637603 ],\n",
       "       [ 5.5587826],\n",
       "       [ 5.2152653],\n",
       "       [ 4.384945 ],\n",
       "       [ 5.5392475],\n",
       "       [ 1.9626235],\n",
       "       [ 2.9042628],\n",
       "       [ 6.148305 ],\n",
       "       [ 1.9959366],\n",
       "       [ 4.1831336],\n",
       "       [ 6.6491203],\n",
       "       [ 3.3918092],\n",
       "       [ 7.8425694],\n",
       "       [ 3.5770662],\n",
       "       [ 4.5741053],\n",
       "       [ 4.964256 ],\n",
       "       [ 4.277465 ],\n",
       "       [ 3.932028 ],\n",
       "       [ 4.888645 ],\n",
       "       [ 3.644017 ],\n",
       "       [ 5.526493 ],\n",
       "       [ 2.868736 ],\n",
       "       [ 4.610566 ],\n",
       "       [ 3.3114426],\n",
       "       [ 6.014241 ],\n",
       "       [ 3.0908725],\n",
       "       [ 6.663188 ],\n",
       "       [ 3.9933255],\n",
       "       [ 2.959479 ],\n",
       "       [ 6.545654 ],\n",
       "       [ 7.3084054],\n",
       "       [ 5.241666 ],\n",
       "       [ 3.5615108],\n",
       "       [ 7.5061464],\n",
       "       [ 4.808955 ],\n",
       "       [ 5.9970894],\n",
       "       [ 4.6717286],\n",
       "       [ 3.982793 ],\n",
       "       [ 4.8915844],\n",
       "       [ 4.4525137],\n",
       "       [ 3.3498218],\n",
       "       [ 3.2801392],\n",
       "       [ 4.4632335],\n",
       "       [ 4.192264 ],\n",
       "       [ 2.9707391],\n",
       "       [ 5.9343014],\n",
       "       [ 4.6943717],\n",
       "       [ 4.175159 ],\n",
       "       [ 3.3620265],\n",
       "       [ 5.178731 ],\n",
       "       [ 7.561317 ],\n",
       "       [ 4.941504 ],\n",
       "       [ 6.249385 ],\n",
       "       [ 5.066816 ],\n",
       "       [ 4.017659 ],\n",
       "       [ 4.722022 ],\n",
       "       [10.633096 ],\n",
       "       [ 5.3629203],\n",
       "       [ 3.9574168],\n",
       "       [ 4.6746264],\n",
       "       [ 3.3552892],\n",
       "       [ 1.9721198],\n",
       "       [ 3.761431 ],\n",
       "       [ 5.7219524],\n",
       "       [ 5.2176266],\n",
       "       [ 5.7653513],\n",
       "       [ 3.1315022],\n",
       "       [ 5.922858 ],\n",
       "       [ 4.8133636],\n",
       "       [ 6.258131 ],\n",
       "       [ 4.0422163],\n",
       "       [ 4.505044 ],\n",
       "       [ 1.8848016],\n",
       "       [ 4.3687396],\n",
       "       [ 2.9564738],\n",
       "       [ 4.281792 ],\n",
       "       [ 3.5352035],\n",
       "       [ 3.728011 ],\n",
       "       [ 3.9764555],\n",
       "       [ 6.8227425],\n",
       "       [ 4.1432953],\n",
       "       [ 2.134531 ],\n",
       "       [ 2.804336 ],\n",
       "       [ 1.9033968],\n",
       "       [ 4.832332 ],\n",
       "       [ 6.297261 ],\n",
       "       [ 4.4958835],\n",
       "       [ 3.0248747],\n",
       "       [ 2.3973045],\n",
       "       [ 5.634753 ],\n",
       "       [ 3.824558 ],\n",
       "       [ 7.1135435],\n",
       "       [ 3.8526416],\n",
       "       [ 4.546587 ],\n",
       "       [ 5.100152 ],\n",
       "       [ 4.9297066],\n",
       "       [ 5.389339 ],\n",
       "       [ 4.541991 ],\n",
       "       [ 4.7436304],\n",
       "       [ 5.25109  ],\n",
       "       [ 3.9582422]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object representative_dataset at 0x000002B2E12CCC48>\n"
     ]
    }
   ],
   "source": [
    "def representative_dataset():\n",
    "    for _ in range(100):\n",
    "      data =  x_train\n",
    "      yield [data.astype(np.float32)]\n",
    "\n",
    "print(representative_dataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_QSAR_model_keras_dir\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, \"saved_QSAR_model_keras_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"saved_QSAR_model_keras_dir\")\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('QSARClassifyModel_new.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"QSARClassifyModel_new.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_details:\n",
      " [{'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([1, 8]), 'shape_signature': array([-1,  8]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "output_details:\n",
      " [{'name': 'StatefulPartitionedCall:0', 'index': 8, 'shape': array([1, 1]), 'shape_signature': array([-1,  1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "print('input_details:\\n', input_details)\n",
    "print('output_details:\\n', output_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_np = x_test.to_numpy()\n",
    "y_test_np = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 8]\n",
      "[[2.7158487]]\n",
      "3.16\n"
     ]
    }
   ],
   "source": [
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "print(input_shape)\n",
    "input_data = [x_test_np[0]]\n",
    "#print(input_data)\n",
    "input_data = np.array(input_data, dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)\n",
    "print(y_test_np[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert some hex values into an array for C programming\n",
    "import time, sys\n",
    "\n",
    "# Function to convert some hex values into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "    c_str = \"\"\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += \"#define \" + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    c_str += \"/*\\n Author: Mouli Sankaran \\n\"\n",
    "    c_str += \" CAUTION: This is an auto generated file.\\n DO NOT EDIT OR MAKE ANY CHANGES TO IT.\\n\"\n",
    "\n",
    "# Time stamping of this model data in the generated file\n",
    "    localtime = time.asctime( time.localtime(time.time()) )\n",
    "    c_str += \" This model data was generated on \" + localtime+ '\\n\\n'\n",
    "    print(\"This model data was generated on:\", localtime)\n",
    "\n",
    "# Add information about the verisons of tools and packages used in generating this header file\n",
    "    c_str += \" Tools used:\\n Python:\" + str(sys.version) + \"\\n Numpy:\" \\\n",
    "            + str(np.version.version) + \"\\n TensorFlow:\" + str(sys.version) \\\n",
    "            + \"\\n Keras: \"+ str(tf.keras.__version__) + \"\\n\\n\"\n",
    "    print(\"Tools used: Python:\", sys.version, \"\\n Numpy:\", np.version.version, \\\n",
    "          \"\\n TensorFlow:\", sys.version, \"\\n Keras: \", tf.keras.__version__, \"\\n\\n\")\n",
    "\n",
    "# Training details of the model\n",
    "    c_str += ' Model details are:\\n'\n",
    "    c_str += ' NUM_OF_EPOCHS  = ' + str(NUM_OF_EPOCHS) + '\\n*/\\n'\n",
    "\n",
    "# Generate 'C' constants for the no. of nodes in each layer\n",
    "    c_str +=   'const int ' + 'DENSE1_SIZE' + ' = ' + str(DENSE1_SIZE) + ';\\n'\n",
    "\n",
    "    # Add array length at the top of the file\n",
    "    c_str += '\\nalignas(8) const unsigned int ' + var_name + '_len = '\\\n",
    "            + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'const unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data):\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formating so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "          hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "          hex_str += '\\n'\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n' + format(''.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model data was generated on: Sun Nov 10 23:06:59 2024\n",
      "Tools used: Python: 3.7.16 (default, Jan 17 2023, 16:06:28) [MSC v.1916 64 bit (AMD64)] \n",
      " Numpy: 1.19.5 \n",
      " TensorFlow: 3.7.16 (default, Jan 17 2023, 16:06:28) [MSC v.1916 64 bit (AMD64)] \n",
      " Keras:  2.7.0 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"QSAR_model_esp32_new\" + '.h', 'w') as file:\n",
    "  file.write(hex_to_c_array(tflite_model, \"QSAR_model_esp32_new\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = x_test_np.shape[1]\n",
    "x_train_np = x_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.   ,   0.   ,   0.   , ...,   0.7  ,   0.   ,   0.   ],\n",
       "       [  0.   ,   0.   ,   0.   , ...,   2.   ,   0.   ,   0.   ],\n",
       "       [  9.23 ,  11.   ,   0.   , ...,   2.003,   0.   ,   0.   ],\n",
       "       ...,\n",
       "       [  0.   ,   0.   ,   0.   , ...,   0.723,   0.   ,   0.   ],\n",
       "       [ 45.61 ,  28.269,   0.   , ...,   1.29 ,   1.   ,   0.   ],\n",
       "       [ 74.92 , 138.783,   1.   , ...,   1.14 ,   2.   ,   1.   ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37.3  , 67.828,  1.   ,  0.586,  1.577,  1.56 ,  0.   ,  1.   ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_np[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{ 3.73000e+01  6.78280e+01  1.00000e+00  5.86000e-01  1.57700e+00\\n   1.56000e+00  0.00000e+00  1.00000e+00}\\n { 2.02300e+01  4.26830e+01  1.00000e+00  4.83300e+00  2.91300e+00\\n   1.22900e+00  0.00000e+00  0.00000e+00}\\n { 6.13500e+01  3.60220e+01  2.00000e+00 -2.73000e-01  1.47500e+00\\n   8.76000e-01  2.00000e+00  0.00000e+00}\\n { 1.66750e+02  2.50102e+02  6.00000e+00  1.63200e+00  5.36900e+00\\n   1.14500e+00  6.00000e+00  4.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  3.12400e+00  1.80000e+00\\n   1.41700e+00  0.00000e+00  0.00000e+00}\\n { 9.23000e+00  1.10000e+01  0.00000e+00  4.19000e+00  2.22500e+00\\n   5.80000e-01  0.00000e+00  0.00000e+00}\\n { 1.14190e+02  1.00652e+02  3.00000e+00  8.21000e-01  3.06000e+00\\n   1.08200e+00  6.00000e+00  1.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  4.20300e+00  1.99700e+00\\n   1.62500e+00  0.00000e+00  0.00000e+00}\\n { 2.02300e+01  4.26830e+01  1.00000e+00  3.31400e+00  2.02700e+00\\n   4.09000e-01  0.00000e+00  0.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  4.63400e+00  2.02700e+00\\n   4.61000e-01  0.00000e+00  0.00000e+00}\\n { 4.93300e+01  8.58390e+01  2.00000e+00  3.98800e+00  3.04700e+00\\n   6.94000e-01  1.00000e+00  1.00000e+00}\\n { 2.03060e+02  3.58244e+02  6.00000e+00  9.57000e-01  5.83900e+00\\n   1.34500e+00  0.00000e+00  1.00000e+00}\\n { 2.60200e+01  3.28970e+01  2.00000e+00  2.72900e+00  1.97500e+00\\n   4.62000e-01  1.00000e+00  0.00000e+00}\\n { 2.94600e+01  5.36830e+01  1.00000e+00  3.84200e+00  2.88700e+00\\n   6.14000e-01  0.00000e+00  0.00000e+00}\\n { 1.15140e+02  2.06609e+02  3.00000e+00 -1.16900e+00  2.33300e+00\\n   1.49600e+00  1.00000e+00  3.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  1.67200e+00  1.22500e+00\\n   5.62000e-01  0.00000e+00  0.00000e+00}\\n { 3.73000e+01  6.78280e+01  1.00000e+00  4.54800e+00  3.10100e+00\\n   1.13700e+00  0.00000e+00  1.00000e+00}\\n { 3.36430e+02  5.51098e+02  1.60000e+01 -5.19900e+00  4.30000e+00\\n   1.37300e+00  7.00000e+00  0.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  2.87600e+00  1.85700e+00\\n   6.99000e-01  0.00000e+00  0.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  5.47200e+00  2.70300e+00\\n   5.77000e-01  0.00000e+00  0.00000e+00}\\n { 6.18200e+01  7.33240e+01  0.00000e+00  7.36000e-01  2.40500e+00\\n   1.54600e+00  4.00000e+00  1.00000e+00}\\n { 5.26000e+01  7.22900e+01  0.00000e+00  6.33800e+00  4.16000e+00\\n   1.40200e+00  0.00000e+00  2.00000e+00}\\n { 7.26800e+01  7.33240e+01  1.00000e+00  4.03000e-01  2.35500e+00\\n   1.42400e+00  4.00000e+00  1.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  3.25900e+00  1.97500e+00\\n   1.11100e+00  0.00000e+00  0.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  3.79500e+00  1.97500e+00\\n   6.57000e-01  0.00000e+00  0.00000e+00}\\n { 4.75800e+01  6.21190e+01  0.00000e+00  3.13300e+00  2.32400e+00\\n   4.67000e-01  2.00000e+00  2.00000e+00}\\n { 4.58200e+01  5.07470e+01  0.00000e+00  2.24100e+00  2.09000e+00\\n   9.42000e-01  1.00000e+00  0.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  3.56200e+00  2.11900e+00\\n   1.15000e+00  0.00000e+00  0.00000e+00}\\n { 4.58200e+01  5.07470e+01  0.00000e+00  1.88700e+00  2.04800e+00\\n   8.42000e-01  1.00000e+00  0.00000e+00}\\n { 7.57400e+01  9.36940e+01  3.00000e+00  2.19500e+00  4.36500e+00\\n   1.08600e+00  2.00000e+00  0.00000e+00}\\n { 3.62800e+01  2.21560e+01  0.00000e+00 -3.17000e-01  1.22500e+00\\n   1.33300e+00  0.00000e+00  0.00000e+00}\\n { 6.27300e+01  8.63780e+01  2.00000e+00  2.59000e+00  2.54600e+00\\n   1.27700e+00  5.00000e+00  0.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  2.08100e+00  1.40100e+00\\n   5.01000e-01  0.00000e+00  0.00000e+00}\\n { 1.00390e+02  9.17750e+01  0.00000e+00  2.68200e+00  2.90300e+00\\n   1.82300e+00  1.00000e+00  0.00000e+00}\\n { 1.61300e+01  1.99100e+01  0.00000e+00  1.27200e+00  2.47300e+00\\n   1.25600e+00  2.00000e+00  0.00000e+00}\\n { 4.04600e+01  8.53670e+01  2.00000e+00  5.25500e+00  3.00100e+00\\n   4.85000e-01  0.00000e+00  0.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  2.60800e+00  1.85700e+00\\n   1.00000e+00  0.00000e+00  0.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  1.67200e+00  1.33400e+00\\n   5.62000e-01  0.00000e+00  0.00000e+00}\\n { 2.63000e+01  8.10250e+01  0.00000e+00  5.40000e+00  3.93600e+00\\n   9.91000e-01  0.00000e+00  1.00000e+00}\\n { 1.15410e+02  7.15120e+01  0.00000e+00  2.22300e+00  2.90300e+00\\n   1.29500e+00  1.00000e+00  0.00000e+00}\\n { 1.21260e+02  6.24900e+00  0.00000e+00  4.54000e-01  2.22700e+00\\n   6.77000e-01  2.00000e+00  0.00000e+00}\\n { 4.44500e+01  1.67860e+01  0.00000e+00  1.15700e+00  1.33400e+00\\n   6.85000e-01  1.00000e+00  1.00000e+00}\\n { 8.17000e+01  1.15445e+02  1.00000e+00  5.44000e+00  4.62300e+00\\n   1.51700e+00  1.00000e+00  3.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  3.38600e+00  2.28400e+00\\n   8.05000e-01  0.00000e+00  0.00000e+00}\\n { 5.93200e+01  9.31640e+01  0.00000e+00  4.27900e+00  3.96200e+00\\n   9.07000e-01  1.00000e+00  2.00000e+00}\\n { 2.91000e+01  4.31560e+01  1.00000e+00  3.07600e+00  2.43800e+00\\n   7.73000e-01  1.00000e+00  1.00000e+00}\\n { 4.75600e+01  6.51550e+01  1.00000e+00  1.57000e+00  2.64400e+00\\n   1.37600e+00  1.00000e+00  0.00000e+00}\\n { 2.02300e+01  4.26830e+01  1.00000e+00  3.61700e+00  2.07400e+00\\n   3.40000e-01  0.00000e+00  0.00000e+00}\\n { 2.82400e+01  0.00000e+00  0.00000e+00  2.85100e+00  2.17600e+00\\n   6.56000e-01  0.00000e+00  0.00000e+00}\\n { 1.84600e+01  2.20000e+01  0.00000e+00  1.46600e+00  2.15000e+00\\n   1.22000e+00  0.00000e+00  0.00000e+00}\\n { 6.55700e+01  7.67900e+01  1.00000e+00  4.89000e-01  1.99900e+00\\n   1.53300e+00  0.00000e+00  0.00000e+00}\\n { 1.31400e+01  9.50700e+00  0.00000e+00  2.85900e+00  2.61400e+00\\n   8.27000e-01  0.00000e+00  0.00000e+00}\\n { 4.53400e+01  3.41060e+01  0.00000e+00  1.29700e+00  2.04200e+00\\n   1.93300e+00  0.00000e+00  0.00000e+00}\\n { 2.63000e+01  3.61450e+01  0.00000e+00  9.96000e-01  1.72300e+00\\n   1.80500e+00  0.00000e+00  1.00000e+00}\\n { 4.25200e+01  4.43130e+01  0.00000e+00  4.74600e+00  2.90500e+00\\n   9.16000e-01  0.00000e+00  0.00000e+00}\\n { 1.22220e+02  1.55543e+02  2.00000e+00  1.40600e+00  3.51100e+00\\n   1.45600e+00  5.00000e+00  2.00000e+00}\\n { 2.60200e+01  3.28970e+01  2.00000e+00  2.19300e+00  2.03100e+00\\n   9.92000e-01  1.00000e+00  0.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  2.22600e+00  1.26500e+00\\n   5.70000e-01  0.00000e+00  0.00000e+00}\\n { 8.72800e+01  1.10688e+02  1.00000e+00  2.84300e+00  3.24400e+00\\n   7.45000e-01  3.00000e+00  1.00000e+00}\\n { 5.45700e+01  4.10280e+01  0.00000e+00  5.65000e+00  4.02600e+00\\n   1.57500e+00  0.00000e+00  0.00000e+00}\\n { 3.62600e+01  6.01440e+01  0.00000e+00  3.30000e+00  3.46900e+00\\n   1.10200e+00  2.00000e+00  1.00000e+00}\\n { 5.93200e+01  7.82040e+01  0.00000e+00  4.79400e+00  3.99000e+00\\n   6.45000e-01  1.00000e+00  2.00000e+00}\\n { 4.73600e+01  5.60550e+01  0.00000e+00  3.37300e+00  3.47100e+00\\n   9.92000e-01  3.00000e+00  0.00000e+00}\\n { 8.74300e+01  5.54110e+01  2.00000e+00 -1.22200e+00  1.57500e+00\\n   1.36200e+00  1.00000e+00  0.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  3.37400e+00  2.07500e+00\\n   1.20400e+00  0.00000e+00  0.00000e+00}\\n { 3.24000e+00  3.12400e+00  0.00000e+00  9.14800e+00  5.49700e+00\\n   1.55900e+00  1.00000e+00  0.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  3.95800e+00  2.48800e+00\\n   8.02000e-01  0.00000e+00  0.00000e+00}\\n { 2.60200e+01  3.28970e+01  2.00000e+00  2.12700e+00  1.90800e+00\\n   5.97000e-01  1.00000e+00  0.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  3.47800e+00  1.90800e+00\\n   5.75000e-01  0.00000e+00  0.00000e+00}\\n { 1.70700e+01  2.51450e+01  0.00000e+00  1.44200e+00  1.84000e+00\\n   1.58000e+00  0.00000e+00  0.00000e+00}\\n { 7.46000e+01  1.35657e+02  2.00000e+00 -4.50000e-01  1.82300e+00\\n   1.07500e+00  0.00000e+00  2.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  2.60400e+00  1.47500e+00\\n   5.05000e-01  0.00000e+00  0.00000e+00}\\n { 5.93200e+01  7.82040e+01  0.00000e+00  3.90700e+00  3.93400e+00\\n   9.08000e-01  1.00000e+00  2.00000e+00}\\n { 5.82000e+01  1.16232e+02  2.00000e+00  3.50800e+00  3.28100e+00\\n   8.03000e-01  2.00000e+00  1.00000e+00}\\n { 2.12600e+01  7.38910e+01  1.00000e+00  4.15300e+00  3.38300e+00\\n   1.12100e+00  1.00000e+00  0.00000e+00}\\n { 4.93300e+01  8.58390e+01  2.00000e+00  1.06000e+00  2.27200e+00\\n   1.00700e+00  1.00000e+00  1.00000e+00}\\n { 1.21260e+02  6.24900e+00  0.00000e+00  1.76100e+00  2.66000e+00\\n   7.79000e-01  2.00000e+00  0.00000e+00}\\n { 4.80200e+01  6.11660e+01  2.00000e+00  2.58900e+00  2.91900e+00\\n   8.26000e-01  2.00000e+00  0.00000e+00}\\n { 3.55300e+01  4.71450e+01  0.00000e+00  4.57900e+00  3.87500e+00\\n   9.27000e-01  0.00000e+00  1.00000e+00}\\n { 2.02300e+01  4.26830e+01  1.00000e+00  1.94000e+00  1.99700e+00\\n   1.61000e+00  0.00000e+00  0.00000e+00}\\n { 3.83300e+01  5.41560e+01  1.00000e+00  2.39100e+00  2.61200e+00\\n   1.29300e+00  1.00000e+00  0.00000e+00}\\n { 1.16670e+02  1.80366e+02  4.00000e+00 -1.96000e+00  2.02100e+00\\n   1.83000e+00  1.00000e+00  1.00000e+00}\\n { 4.58200e+01  5.07470e+01  0.00000e+00  3.11000e+00  2.14100e+00\\n   4.78000e-01  1.00000e+00  0.00000e+00}\\n { 8.43200e+01  1.25108e+02  2.00000e+00  1.39000e-01  2.74800e+00\\n   1.26700e+00  2.00000e+00  1.00000e+00}\\n { 4.58200e+01  5.07470e+01  0.00000e+00  2.84200e+00  2.15400e+00\\n   7.26000e-01  1.00000e+00  0.00000e+00}\\n { 5.75300e+01  1.10512e+02  2.00000e+00  1.64300e+00  2.09000e+00\\n   8.14000e-01  0.00000e+00  1.00000e+00}\\n { 2.02300e+01  4.26830e+01  1.00000e+00  1.85900e+00  1.90800e+00\\n   9.49000e-01  0.00000e+00  0.00000e+00}\\n { 7.45700e+01  1.47153e+02  2.00000e+00  1.78900e+00  3.37300e+00\\n   1.17500e+00  3.00000e+00  1.00000e+00}\\n { 8.89000e+01  3.35710e+01  0.00000e+00  4.40500e+00  2.45600e+00\\n   5.68000e-01  2.00000e+00  2.00000e+00}\\n { 1.19330e+02  1.34931e+02  0.00000e+00  2.89000e-01  2.58700e+00\\n   1.98900e+00  2.00000e+00  0.00000e+00}\\n { 6.57200e+01  1.01271e+02  2.00000e+00 -2.34000e-01  1.97500e+00\\n   1.09100e+00  2.00000e+00  1.00000e+00}\\n { 3.86900e+01  6.46830e+01  1.00000e+00  2.24000e-01  2.10800e+00\\n   1.90400e+00  0.00000e+00  0.00000e+00}\\n { 2.37900e+01  3.10590e+01  0.00000e+00  9.40000e-02  1.33400e+00\\n   1.11500e+00  1.00000e+00  1.00000e+00}\\n { 1.38450e+02  2.81245e+02  6.00000e+00  1.44200e+00  3.87800e+00\\n   1.20500e+00  0.00000e+00  0.00000e+00}\\n { 4.94100e+01  7.11800e+01  1.00000e+00  4.76100e+00  3.53700e+00\\n   1.14700e+00  2.00000e+00  2.00000e+00}\\n { 1.03350e+02  1.61259e+02  2.00000e+00  2.40100e+00  3.05300e+00\\n   7.98000e-01  1.00000e+00  0.00000e+00}\\n { 0.00000e+00  1.49600e+01  0.00000e+00  2.02600e+00  1.26500e+00\\n   7.22000e-01  0.00000e+00  0.00000e+00}\\n { 1.25300e+01  1.10000e+01  0.00000e+00  4.08000e-01  1.68900e+00\\n   1.03800e+00  0.00000e+00  0.00000e+00}\\n { 1.07650e+02  1.78917e+02  0.00000e+00  4.21900e+00  3.94500e+00\\n   1.07800e+00  1.00000e+00  2.00000e+00}\\n { 7.45700e+01  1.32193e+02  2.00000e+00  1.67100e+00  3.50100e+00\\n   1.09500e+00  3.00000e+00  1.00000e+00}\\n { 1.33440e+02  7.51930e+01  0.00000e+00  9.06000e-01  3.03400e+00\\n   9.41000e-01  3.00000e+00  1.00000e+00}\\n { 3.02100e+01  3.61450e+01  0.00000e+00  2.26500e+00  2.34400e+00\\n   9.23000e-01  0.00000e+00  1.00000e+00}\\n { 5.26000e+01  7.22900e+01  0.00000e+00  2.57700e+00  2.70700e+00\\n   1.31300e+00  0.00000e+00  2.00000e+00}\\n { 2.02300e+01  4.26830e+01  1.00000e+00  3.90900e+00  2.12000e+00\\n   2.81000e-01  0.00000e+00  0.00000e+00}\\n { 1.11560e+02  9.94000e+01  2.00000e+00  6.58000e-01  3.42600e+00\\n   1.18900e+00  4.00000e+00  0.00000e+00}\\n { 7.78200e+01  9.93650e+01  4.00000e+00  3.04800e+00  2.81300e+00\\n   7.91000e-01  4.00000e+00  0.00000e+00}\\n { 2.94600e+01  5.36830e+01  1.00000e+00  3.02800e+00  2.22500e+00\\n   5.48000e-01  0.00000e+00  0.00000e+00}\\n { 0.00000e+00  0.00000e+00  0.00000e+00  3.26700e+00  2.31800e+00\\n   9.63000e-01  0.00000e+00  0.00000e+00}\\n { 9.23500e+01  1.74455e+02  1.00000e+00  3.84000e+00  3.37300e+00\\n   8.02000e-01  1.00000e+00  1.00000e+00}\\n { 4.15700e+01  5.72800e+01  1.00000e+00  1.83800e+00  2.61100e+00\\n   1.17500e+00  2.00000e+00  0.00000e+00}}'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[ 3.73000e+01  6.78280e+01  1.00000e+00  5.86000e-01  1.57700e+00\\n   1.56000e+00  0.00000e+00  1.00000e+00]\\n [ 2.02300e+01  4.26830e+01  1.00000e+00  4.83300e+00  2.91300e+00\\n   1.22900e+00  0.00000e+00  0.00000e+00]\\n [ 6.13500e+01  3.60220e+01  2.00000e+00 -2.73000e-01  1.47500e+00\\n   8.76000e-01  2.00000e+00  0.00000e+00]\\n [ 1.66750e+02  2.50102e+02  6.00000e+00  1.63200e+00  5.36900e+00\\n   1.14500e+00  6.00000e+00  4.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  3.12400e+00  1.80000e+00\\n   1.41700e+00  0.00000e+00  0.00000e+00]\\n [ 9.23000e+00  1.10000e+01  0.00000e+00  4.19000e+00  2.22500e+00\\n   5.80000e-01  0.00000e+00  0.00000e+00]\\n [ 1.14190e+02  1.00652e+02  3.00000e+00  8.21000e-01  3.06000e+00\\n   1.08200e+00  6.00000e+00  1.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  4.20300e+00  1.99700e+00\\n   1.62500e+00  0.00000e+00  0.00000e+00]\\n [ 2.02300e+01  4.26830e+01  1.00000e+00  3.31400e+00  2.02700e+00\\n   4.09000e-01  0.00000e+00  0.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  4.63400e+00  2.02700e+00\\n   4.61000e-01  0.00000e+00  0.00000e+00]\\n [ 4.93300e+01  8.58390e+01  2.00000e+00  3.98800e+00  3.04700e+00\\n   6.94000e-01  1.00000e+00  1.00000e+00]\\n [ 2.03060e+02  3.58244e+02  6.00000e+00  9.57000e-01  5.83900e+00\\n   1.34500e+00  0.00000e+00  1.00000e+00]\\n [ 2.60200e+01  3.28970e+01  2.00000e+00  2.72900e+00  1.97500e+00\\n   4.62000e-01  1.00000e+00  0.00000e+00]\\n [ 2.94600e+01  5.36830e+01  1.00000e+00  3.84200e+00  2.88700e+00\\n   6.14000e-01  0.00000e+00  0.00000e+00]\\n [ 1.15140e+02  2.06609e+02  3.00000e+00 -1.16900e+00  2.33300e+00\\n   1.49600e+00  1.00000e+00  3.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  1.67200e+00  1.22500e+00\\n   5.62000e-01  0.00000e+00  0.00000e+00]\\n [ 3.73000e+01  6.78280e+01  1.00000e+00  4.54800e+00  3.10100e+00\\n   1.13700e+00  0.00000e+00  1.00000e+00]\\n [ 3.36430e+02  5.51098e+02  1.60000e+01 -5.19900e+00  4.30000e+00\\n   1.37300e+00  7.00000e+00  0.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  2.87600e+00  1.85700e+00\\n   6.99000e-01  0.00000e+00  0.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  5.47200e+00  2.70300e+00\\n   5.77000e-01  0.00000e+00  0.00000e+00]\\n [ 6.18200e+01  7.33240e+01  0.00000e+00  7.36000e-01  2.40500e+00\\n   1.54600e+00  4.00000e+00  1.00000e+00]\\n [ 5.26000e+01  7.22900e+01  0.00000e+00  6.33800e+00  4.16000e+00\\n   1.40200e+00  0.00000e+00  2.00000e+00]\\n [ 7.26800e+01  7.33240e+01  1.00000e+00  4.03000e-01  2.35500e+00\\n   1.42400e+00  4.00000e+00  1.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  3.25900e+00  1.97500e+00\\n   1.11100e+00  0.00000e+00  0.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  3.79500e+00  1.97500e+00\\n   6.57000e-01  0.00000e+00  0.00000e+00]\\n [ 4.75800e+01  6.21190e+01  0.00000e+00  3.13300e+00  2.32400e+00\\n   4.67000e-01  2.00000e+00  2.00000e+00]\\n [ 4.58200e+01  5.07470e+01  0.00000e+00  2.24100e+00  2.09000e+00\\n   9.42000e-01  1.00000e+00  0.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  3.56200e+00  2.11900e+00\\n   1.15000e+00  0.00000e+00  0.00000e+00]\\n [ 4.58200e+01  5.07470e+01  0.00000e+00  1.88700e+00  2.04800e+00\\n   8.42000e-01  1.00000e+00  0.00000e+00]\\n [ 7.57400e+01  9.36940e+01  3.00000e+00  2.19500e+00  4.36500e+00\\n   1.08600e+00  2.00000e+00  0.00000e+00]\\n [ 3.62800e+01  2.21560e+01  0.00000e+00 -3.17000e-01  1.22500e+00\\n   1.33300e+00  0.00000e+00  0.00000e+00]\\n [ 6.27300e+01  8.63780e+01  2.00000e+00  2.59000e+00  2.54600e+00\\n   1.27700e+00  5.00000e+00  0.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  2.08100e+00  1.40100e+00\\n   5.01000e-01  0.00000e+00  0.00000e+00]\\n [ 1.00390e+02  9.17750e+01  0.00000e+00  2.68200e+00  2.90300e+00\\n   1.82300e+00  1.00000e+00  0.00000e+00]\\n [ 1.61300e+01  1.99100e+01  0.00000e+00  1.27200e+00  2.47300e+00\\n   1.25600e+00  2.00000e+00  0.00000e+00]\\n [ 4.04600e+01  8.53670e+01  2.00000e+00  5.25500e+00  3.00100e+00\\n   4.85000e-01  0.00000e+00  0.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  2.60800e+00  1.85700e+00\\n   1.00000e+00  0.00000e+00  0.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  1.67200e+00  1.33400e+00\\n   5.62000e-01  0.00000e+00  0.00000e+00]\\n [ 2.63000e+01  8.10250e+01  0.00000e+00  5.40000e+00  3.93600e+00\\n   9.91000e-01  0.00000e+00  1.00000e+00]\\n [ 1.15410e+02  7.15120e+01  0.00000e+00  2.22300e+00  2.90300e+00\\n   1.29500e+00  1.00000e+00  0.00000e+00]\\n [ 1.21260e+02  6.24900e+00  0.00000e+00  4.54000e-01  2.22700e+00\\n   6.77000e-01  2.00000e+00  0.00000e+00]\\n [ 4.44500e+01  1.67860e+01  0.00000e+00  1.15700e+00  1.33400e+00\\n   6.85000e-01  1.00000e+00  1.00000e+00]\\n [ 8.17000e+01  1.15445e+02  1.00000e+00  5.44000e+00  4.62300e+00\\n   1.51700e+00  1.00000e+00  3.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  3.38600e+00  2.28400e+00\\n   8.05000e-01  0.00000e+00  0.00000e+00]\\n [ 5.93200e+01  9.31640e+01  0.00000e+00  4.27900e+00  3.96200e+00\\n   9.07000e-01  1.00000e+00  2.00000e+00]\\n [ 2.91000e+01  4.31560e+01  1.00000e+00  3.07600e+00  2.43800e+00\\n   7.73000e-01  1.00000e+00  1.00000e+00]\\n [ 4.75600e+01  6.51550e+01  1.00000e+00  1.57000e+00  2.64400e+00\\n   1.37600e+00  1.00000e+00  0.00000e+00]\\n [ 2.02300e+01  4.26830e+01  1.00000e+00  3.61700e+00  2.07400e+00\\n   3.40000e-01  0.00000e+00  0.00000e+00]\\n [ 2.82400e+01  0.00000e+00  0.00000e+00  2.85100e+00  2.17600e+00\\n   6.56000e-01  0.00000e+00  0.00000e+00]\\n [ 1.84600e+01  2.20000e+01  0.00000e+00  1.46600e+00  2.15000e+00\\n   1.22000e+00  0.00000e+00  0.00000e+00]\\n [ 6.55700e+01  7.67900e+01  1.00000e+00  4.89000e-01  1.99900e+00\\n   1.53300e+00  0.00000e+00  0.00000e+00]\\n [ 1.31400e+01  9.50700e+00  0.00000e+00  2.85900e+00  2.61400e+00\\n   8.27000e-01  0.00000e+00  0.00000e+00]\\n [ 4.53400e+01  3.41060e+01  0.00000e+00  1.29700e+00  2.04200e+00\\n   1.93300e+00  0.00000e+00  0.00000e+00]\\n [ 2.63000e+01  3.61450e+01  0.00000e+00  9.96000e-01  1.72300e+00\\n   1.80500e+00  0.00000e+00  1.00000e+00]\\n [ 4.25200e+01  4.43130e+01  0.00000e+00  4.74600e+00  2.90500e+00\\n   9.16000e-01  0.00000e+00  0.00000e+00]\\n [ 1.22220e+02  1.55543e+02  2.00000e+00  1.40600e+00  3.51100e+00\\n   1.45600e+00  5.00000e+00  2.00000e+00]\\n [ 2.60200e+01  3.28970e+01  2.00000e+00  2.19300e+00  2.03100e+00\\n   9.92000e-01  1.00000e+00  0.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  2.22600e+00  1.26500e+00\\n   5.70000e-01  0.00000e+00  0.00000e+00]\\n [ 8.72800e+01  1.10688e+02  1.00000e+00  2.84300e+00  3.24400e+00\\n   7.45000e-01  3.00000e+00  1.00000e+00]\\n [ 5.45700e+01  4.10280e+01  0.00000e+00  5.65000e+00  4.02600e+00\\n   1.57500e+00  0.00000e+00  0.00000e+00]\\n [ 3.62600e+01  6.01440e+01  0.00000e+00  3.30000e+00  3.46900e+00\\n   1.10200e+00  2.00000e+00  1.00000e+00]\\n [ 5.93200e+01  7.82040e+01  0.00000e+00  4.79400e+00  3.99000e+00\\n   6.45000e-01  1.00000e+00  2.00000e+00]\\n [ 4.73600e+01  5.60550e+01  0.00000e+00  3.37300e+00  3.47100e+00\\n   9.92000e-01  3.00000e+00  0.00000e+00]\\n [ 8.74300e+01  5.54110e+01  2.00000e+00 -1.22200e+00  1.57500e+00\\n   1.36200e+00  1.00000e+00  0.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  3.37400e+00  2.07500e+00\\n   1.20400e+00  0.00000e+00  0.00000e+00]\\n [ 3.24000e+00  3.12400e+00  0.00000e+00  9.14800e+00  5.49700e+00\\n   1.55900e+00  1.00000e+00  0.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  3.95800e+00  2.48800e+00\\n   8.02000e-01  0.00000e+00  0.00000e+00]\\n [ 2.60200e+01  3.28970e+01  2.00000e+00  2.12700e+00  1.90800e+00\\n   5.97000e-01  1.00000e+00  0.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  3.47800e+00  1.90800e+00\\n   5.75000e-01  0.00000e+00  0.00000e+00]\\n [ 1.70700e+01  2.51450e+01  0.00000e+00  1.44200e+00  1.84000e+00\\n   1.58000e+00  0.00000e+00  0.00000e+00]\\n [ 7.46000e+01  1.35657e+02  2.00000e+00 -4.50000e-01  1.82300e+00\\n   1.07500e+00  0.00000e+00  2.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  2.60400e+00  1.47500e+00\\n   5.05000e-01  0.00000e+00  0.00000e+00]\\n [ 5.93200e+01  7.82040e+01  0.00000e+00  3.90700e+00  3.93400e+00\\n   9.08000e-01  1.00000e+00  2.00000e+00]\\n [ 5.82000e+01  1.16232e+02  2.00000e+00  3.50800e+00  3.28100e+00\\n   8.03000e-01  2.00000e+00  1.00000e+00]\\n [ 2.12600e+01  7.38910e+01  1.00000e+00  4.15300e+00  3.38300e+00\\n   1.12100e+00  1.00000e+00  0.00000e+00]\\n [ 4.93300e+01  8.58390e+01  2.00000e+00  1.06000e+00  2.27200e+00\\n   1.00700e+00  1.00000e+00  1.00000e+00]\\n [ 1.21260e+02  6.24900e+00  0.00000e+00  1.76100e+00  2.66000e+00\\n   7.79000e-01  2.00000e+00  0.00000e+00]\\n [ 4.80200e+01  6.11660e+01  2.00000e+00  2.58900e+00  2.91900e+00\\n   8.26000e-01  2.00000e+00  0.00000e+00]\\n [ 3.55300e+01  4.71450e+01  0.00000e+00  4.57900e+00  3.87500e+00\\n   9.27000e-01  0.00000e+00  1.00000e+00]\\n [ 2.02300e+01  4.26830e+01  1.00000e+00  1.94000e+00  1.99700e+00\\n   1.61000e+00  0.00000e+00  0.00000e+00]\\n [ 3.83300e+01  5.41560e+01  1.00000e+00  2.39100e+00  2.61200e+00\\n   1.29300e+00  1.00000e+00  0.00000e+00]\\n [ 1.16670e+02  1.80366e+02  4.00000e+00 -1.96000e+00  2.02100e+00\\n   1.83000e+00  1.00000e+00  1.00000e+00]\\n [ 4.58200e+01  5.07470e+01  0.00000e+00  3.11000e+00  2.14100e+00\\n   4.78000e-01  1.00000e+00  0.00000e+00]\\n [ 8.43200e+01  1.25108e+02  2.00000e+00  1.39000e-01  2.74800e+00\\n   1.26700e+00  2.00000e+00  1.00000e+00]\\n [ 4.58200e+01  5.07470e+01  0.00000e+00  2.84200e+00  2.15400e+00\\n   7.26000e-01  1.00000e+00  0.00000e+00]\\n [ 5.75300e+01  1.10512e+02  2.00000e+00  1.64300e+00  2.09000e+00\\n   8.14000e-01  0.00000e+00  1.00000e+00]\\n [ 2.02300e+01  4.26830e+01  1.00000e+00  1.85900e+00  1.90800e+00\\n   9.49000e-01  0.00000e+00  0.00000e+00]\\n [ 7.45700e+01  1.47153e+02  2.00000e+00  1.78900e+00  3.37300e+00\\n   1.17500e+00  3.00000e+00  1.00000e+00]\\n [ 8.89000e+01  3.35710e+01  0.00000e+00  4.40500e+00  2.45600e+00\\n   5.68000e-01  2.00000e+00  2.00000e+00]\\n [ 1.19330e+02  1.34931e+02  0.00000e+00  2.89000e-01  2.58700e+00\\n   1.98900e+00  2.00000e+00  0.00000e+00]\\n [ 6.57200e+01  1.01271e+02  2.00000e+00 -2.34000e-01  1.97500e+00\\n   1.09100e+00  2.00000e+00  1.00000e+00]\\n [ 3.86900e+01  6.46830e+01  1.00000e+00  2.24000e-01  2.10800e+00\\n   1.90400e+00  0.00000e+00  0.00000e+00]\\n [ 2.37900e+01  3.10590e+01  0.00000e+00  9.40000e-02  1.33400e+00\\n   1.11500e+00  1.00000e+00  1.00000e+00]\\n [ 1.38450e+02  2.81245e+02  6.00000e+00  1.44200e+00  3.87800e+00\\n   1.20500e+00  0.00000e+00  0.00000e+00]\\n [ 4.94100e+01  7.11800e+01  1.00000e+00  4.76100e+00  3.53700e+00\\n   1.14700e+00  2.00000e+00  2.00000e+00]\\n [ 1.03350e+02  1.61259e+02  2.00000e+00  2.40100e+00  3.05300e+00\\n   7.98000e-01  1.00000e+00  0.00000e+00]\\n [ 0.00000e+00  1.49600e+01  0.00000e+00  2.02600e+00  1.26500e+00\\n   7.22000e-01  0.00000e+00  0.00000e+00]\\n [ 1.25300e+01  1.10000e+01  0.00000e+00  4.08000e-01  1.68900e+00\\n   1.03800e+00  0.00000e+00  0.00000e+00]\\n [ 1.07650e+02  1.78917e+02  0.00000e+00  4.21900e+00  3.94500e+00\\n   1.07800e+00  1.00000e+00  2.00000e+00]\\n [ 7.45700e+01  1.32193e+02  2.00000e+00  1.67100e+00  3.50100e+00\\n   1.09500e+00  3.00000e+00  1.00000e+00]\\n [ 1.33440e+02  7.51930e+01  0.00000e+00  9.06000e-01  3.03400e+00\\n   9.41000e-01  3.00000e+00  1.00000e+00]\\n [ 3.02100e+01  3.61450e+01  0.00000e+00  2.26500e+00  2.34400e+00\\n   9.23000e-01  0.00000e+00  1.00000e+00]\\n [ 5.26000e+01  7.22900e+01  0.00000e+00  2.57700e+00  2.70700e+00\\n   1.31300e+00  0.00000e+00  2.00000e+00]\\n [ 2.02300e+01  4.26830e+01  1.00000e+00  3.90900e+00  2.12000e+00\\n   2.81000e-01  0.00000e+00  0.00000e+00]\\n [ 1.11560e+02  9.94000e+01  2.00000e+00  6.58000e-01  3.42600e+00\\n   1.18900e+00  4.00000e+00  0.00000e+00]\\n [ 7.78200e+01  9.93650e+01  4.00000e+00  3.04800e+00  2.81300e+00\\n   7.91000e-01  4.00000e+00  0.00000e+00]\\n [ 2.94600e+01  5.36830e+01  1.00000e+00  3.02800e+00  2.22500e+00\\n   5.48000e-01  0.00000e+00  0.00000e+00]\\n [ 0.00000e+00  0.00000e+00  0.00000e+00  3.26700e+00  2.31800e+00\\n   9.63000e-01  0.00000e+00  0.00000e+00]\\n [ 9.23500e+01  1.74455e+02  1.00000e+00  3.84000e+00  3.37300e+00\\n   8.02000e-01  1.00000e+00  1.00000e+00]\\n [ 4.15700e+01  5.72800e+01  1.00000e+00  1.83800e+00  2.61100e+00\\n   1.17500e+00  2.00000e+00  0.00000e+00]]'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(x_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_array_string_2d(data, name):\n",
    "    string = \"\"\n",
    "    string += f'float {name}[{data.shape[0]}][{data.shape[1]}] = \\n' +\"{\"\n",
    "    for i in range(x_test_np.shape[0]):\n",
    "        string += \"{\"\n",
    "        for j in range(x_test_np.shape[1]-1):\n",
    "            string += str(x_test_np[i][j]) + \", \"\n",
    "        string += str(x_test_np[i][x_test_np.shape[1]-1])\n",
    "        string += \"},\\n\"\n",
    "    string += \"};\"\n",
    "    return string\n",
    "\n",
    "def generate_array_string_1d(data, name):\n",
    "    string = \"\"\n",
    "    string += f'float {name}[{data.shape[0]}] = '\n",
    "    string += \"{\"\n",
    "    for i in range(data.shape[0]-1):\n",
    "        string += str(data[i]) + \", \"\n",
    "    string += str(data[data.shape[0]-1])\n",
    "    string += \"};\"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float y_test0[110] = {3.16, 6.36, 3.392, 4.27, 4.34, 7.012, 2.533, 5.47, 5.461, 4.874, 4.121, 4.566, 5.437, 6.109, 2.736, 3.28, 4.56, 3.077, 3.918, 6.209, 3.028, 6.194, 2.572, 4.53, 5.42, 6.21, 4.14, 4.317, 3.615, 5.132, 0.495, 3.601, 3.485, 9.141, 4.733, 4.838, 3.002, 2.642, 8.112, 8.15, 6.059, 5.418, 4.85, 4.147, 9.133, 4.714, 4.91, 6.11, 3.357, 2.937, 7.699, 4.995, 2.829, 2.827, 4.711, 2.593, 4.179, 3.718, 4.022, 7.1, 4.92, 9.945, 4.943, 6.618, 3.792, 5.96, 3.575, 5.19, 4.787, 4.039, 2.764, 3.46, 9.063, 7.802, 5.783, 4.216, 5.976, 4.234, 8.067, 3.226, 6.32, 3.908, 4.62, 2.359, 4.268, 3.068, 3.863, 3.432, 6.399, 3.339, 3.716, 1.888, 3.78, 5.291, 4.307, 4.14, 3.577, 3.587, 5.03, 3.742, 7.858, 4.032, 3.603, 5.56, 2.685, 4.632, 6.176, 4.1, 4.257, 3.155};\\n'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_array_string_1d(y_test_np, 'y_test0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float x_test0[110][8] = \\n{{37.3, 67.828, 1.0, 0.586, 1.577, 1.56, 0.0, 1.0},\\n{20.23, 42.683, 1.0, 4.833, 2.913, 1.229, 0.0, 0.0},\\n{61.35, 36.022, 2.0, -0.273, 1.475, 0.876, 2.0, 0.0},\\n{166.75, 250.102, 6.0, 1.632, 5.369, 1.145, 6.0, 4.0},\\n{0.0, 0.0, 0.0, 3.124, 1.8, 1.417, 0.0, 0.0},\\n{9.23, 11.0, 0.0, 4.19, 2.225, 0.58, 0.0, 0.0},\\n{114.19, 100.652, 3.0, 0.821, 3.06, 1.082, 6.0, 1.0},\\n{0.0, 0.0, 0.0, 4.203, 1.997, 1.625, 0.0, 0.0},\\n{20.23, 42.683, 1.0, 3.314, 2.027, 0.409, 0.0, 0.0},\\n{0.0, 0.0, 0.0, 4.634, 2.027, 0.461, 0.0, 0.0},\\n{49.33, 85.839, 2.0, 3.988, 3.047, 0.694, 1.0, 1.0},\\n{203.06, 358.244, 6.0, 0.957, 5.839, 1.345, 0.0, 1.0},\\n{26.02, 32.897, 2.0, 2.729, 1.975, 0.462, 1.0, 0.0},\\n{29.46, 53.683, 1.0, 3.842, 2.887, 0.614, 0.0, 0.0},\\n{115.14, 206.609, 3.0, -1.169, 2.333, 1.496, 1.0, 3.0},\\n{0.0, 0.0, 0.0, 1.672, 1.225, 0.562, 0.0, 0.0},\\n{37.3, 67.828, 1.0, 4.548, 3.101, 1.137, 0.0, 1.0},\\n{336.43, 551.098, 16.0, -5.199, 4.3, 1.373, 7.0, 0.0},\\n{0.0, 0.0, 0.0, 2.876, 1.857, 0.699, 0.0, 0.0},\\n{0.0, 0.0, 0.0, 5.472, 2.703, 0.577, 0.0, 0.0},\\n{61.82, 73.324, 0.0, 0.736, 2.405, 1.546, 4.0, 1.0},\\n{52.6, 72.29, 0.0, 6.338, 4.16, 1.402, 0.0, 2.0},\\n{72.68, 73.324, 1.0, 0.403, 2.355, 1.424, 4.0, 1.0},\\n{0.0, 0.0, 0.0, 3.259, 1.975, 1.111, 0.0, 0.0},\\n{0.0, 0.0, 0.0, 3.795, 1.975, 0.657, 0.0, 0.0},\\n{47.58, 62.119, 0.0, 3.133, 2.324, 0.467, 2.0, 2.0},\\n{45.82, 50.747, 0.0, 2.241, 2.09, 0.942, 1.0, 0.0},\\n{0.0, 0.0, 0.0, 3.562, 2.119, 1.15, 0.0, 0.0},\\n{45.82, 50.747, 0.0, 1.887, 2.048, 0.842, 1.0, 0.0},\\n{75.74, 93.694, 3.0, 2.195, 4.365, 1.086, 2.0, 0.0},\\n{36.28, 22.156, 0.0, -0.317, 1.225, 1.333, 0.0, 0.0},\\n{62.73, 86.378, 2.0, 2.59, 2.546, 1.277, 5.0, 0.0},\\n{0.0, 0.0, 0.0, 2.081, 1.401, 0.501, 0.0, 0.0},\\n{100.39, 91.775, 0.0, 2.682, 2.903, 1.823, 1.0, 0.0},\\n{16.13, 19.91, 0.0, 1.272, 2.473, 1.256, 2.0, 0.0},\\n{40.46, 85.367, 2.0, 5.255, 3.001, 0.485, 0.0, 0.0},\\n{0.0, 0.0, 0.0, 2.608, 1.857, 1.0, 0.0, 0.0},\\n{0.0, 0.0, 0.0, 1.672, 1.334, 0.562, 0.0, 0.0},\\n{26.3, 81.025, 0.0, 5.4, 3.936, 0.991, 0.0, 1.0},\\n{115.41, 71.512, 0.0, 2.223, 2.903, 1.295, 1.0, 0.0},\\n{121.26, 6.249, 0.0, 0.454, 2.227, 0.677, 2.0, 0.0},\\n{44.45, 16.786, 0.0, 1.157, 1.334, 0.685, 1.0, 1.0},\\n{81.7, 115.445, 1.0, 5.44, 4.623, 1.517, 1.0, 3.0},\\n{0.0, 0.0, 0.0, 3.386, 2.284, 0.805, 0.0, 0.0},\\n{59.32, 93.164, 0.0, 4.279, 3.962, 0.907, 1.0, 2.0},\\n{29.1, 43.156, 1.0, 3.076, 2.438, 0.773, 1.0, 1.0},\\n{47.56, 65.155, 1.0, 1.57, 2.644, 1.376, 1.0, 0.0},\\n{20.23, 42.683, 1.0, 3.617, 2.074, 0.34, 0.0, 0.0},\\n{28.24, 0.0, 0.0, 2.851, 2.176, 0.656, 0.0, 0.0},\\n{18.46, 22.0, 0.0, 1.466, 2.15, 1.22, 0.0, 0.0},\\n{65.57, 76.79, 1.0, 0.489, 1.999, 1.533, 0.0, 0.0},\\n{13.14, 9.507, 0.0, 2.859, 2.614, 0.827, 0.0, 0.0},\\n{45.34, 34.106, 0.0, 1.297, 2.042, 1.933, 0.0, 0.0},\\n{26.3, 36.145, 0.0, 0.996, 1.723, 1.805, 0.0, 1.0},\\n{42.52, 44.313, 0.0, 4.746, 2.905, 0.916, 0.0, 0.0},\\n{122.22, 155.543, 2.0, 1.406, 3.511, 1.456, 5.0, 2.0},\\n{26.02, 32.897, 2.0, 2.193, 2.031, 0.992, 1.0, 0.0},\\n{0.0, 0.0, 0.0, 2.226, 1.265, 0.57, 0.0, 0.0},\\n{87.28, 110.688, 1.0, 2.843, 3.244, 0.745, 3.0, 1.0},\\n{54.57, 41.028, 0.0, 5.65, 4.026, 1.575, 0.0, 0.0},\\n{36.26, 60.144, 0.0, 3.3, 3.469, 1.102, 2.0, 1.0},\\n{59.32, 78.204, 0.0, 4.794, 3.99, 0.645, 1.0, 2.0},\\n{47.36, 56.055, 0.0, 3.373, 3.471, 0.992, 3.0, 0.0},\\n{87.43, 55.411, 2.0, -1.222, 1.575, 1.362, 1.0, 0.0},\\n{0.0, 0.0, 0.0, 3.374, 2.075, 1.204, 0.0, 0.0},\\n{3.24, 3.124, 0.0, 9.148, 5.497, 1.559, 1.0, 0.0},\\n{0.0, 0.0, 0.0, 3.958, 2.488, 0.802, 0.0, 0.0},\\n{26.02, 32.897, 2.0, 2.127, 1.908, 0.597, 1.0, 0.0},\\n{0.0, 0.0, 0.0, 3.478, 1.908, 0.575, 0.0, 0.0},\\n{17.07, 25.145, 0.0, 1.442, 1.84, 1.58, 0.0, 0.0},\\n{74.6, 135.657, 2.0, -0.45, 1.823, 1.075, 0.0, 2.0},\\n{0.0, 0.0, 0.0, 2.604, 1.475, 0.505, 0.0, 0.0},\\n{59.32, 78.204, 0.0, 3.907, 3.934, 0.908, 1.0, 2.0},\\n{58.2, 116.232, 2.0, 3.508, 3.281, 0.803, 2.0, 1.0},\\n{21.26, 73.891, 1.0, 4.153, 3.383, 1.121, 1.0, 0.0},\\n{49.33, 85.839, 2.0, 1.06, 2.272, 1.007, 1.0, 1.0},\\n{121.26, 6.249, 0.0, 1.761, 2.66, 0.779, 2.0, 0.0},\\n{48.02, 61.166, 2.0, 2.589, 2.919, 0.826, 2.0, 0.0},\\n{35.53, 47.145, 0.0, 4.579, 3.875, 0.927, 0.0, 1.0},\\n{20.23, 42.683, 1.0, 1.94, 1.997, 1.61, 0.0, 0.0},\\n{38.33, 54.156, 1.0, 2.391, 2.612, 1.293, 1.0, 0.0},\\n{116.67, 180.366, 4.0, -1.96, 2.021, 1.83, 1.0, 1.0},\\n{45.82, 50.747, 0.0, 3.11, 2.141, 0.478, 1.0, 0.0},\\n{84.32, 125.108, 2.0, 0.139, 2.748, 1.267, 2.0, 1.0},\\n{45.82, 50.747, 0.0, 2.842, 2.154, 0.726, 1.0, 0.0},\\n{57.53, 110.512, 2.0, 1.643, 2.09, 0.814, 0.0, 1.0},\\n{20.23, 42.683, 1.0, 1.859, 1.908, 0.949, 0.0, 0.0},\\n{74.57, 147.153, 2.0, 1.789, 3.373, 1.175, 3.0, 1.0},\\n{88.9, 33.571, 0.0, 4.405, 2.456, 0.568, 2.0, 2.0},\\n{119.33, 134.931, 0.0, 0.289, 2.587, 1.989, 2.0, 0.0},\\n{65.72, 101.271, 2.0, -0.234, 1.975, 1.091, 2.0, 1.0},\\n{38.69, 64.683, 1.0, 0.224, 2.108, 1.904, 0.0, 0.0},\\n{23.79, 31.059, 0.0, 0.094, 1.334, 1.115, 1.0, 1.0},\\n{138.45, 281.245, 6.0, 1.442, 3.878, 1.205, 0.0, 0.0},\\n{49.41, 71.18, 1.0, 4.761, 3.537, 1.147, 2.0, 2.0},\\n{103.35, 161.259, 2.0, 2.401, 3.053, 0.798, 1.0, 0.0},\\n{0.0, 14.96, 0.0, 2.026, 1.265, 0.722, 0.0, 0.0},\\n{12.53, 11.0, 0.0, 0.408, 1.689, 1.038, 0.0, 0.0},\\n{107.65, 178.917, 0.0, 4.219, 3.945, 1.078, 1.0, 2.0},\\n{74.57, 132.193, 2.0, 1.671, 3.501, 1.095, 3.0, 1.0},\\n{133.44, 75.193, 0.0, 0.906, 3.034, 0.941, 3.0, 1.0},\\n{30.21, 36.145, 0.0, 2.265, 2.344, 0.923, 0.0, 1.0},\\n{52.6, 72.29, 0.0, 2.577, 2.707, 1.313, 0.0, 2.0},\\n{20.23, 42.683, 1.0, 3.909, 2.12, 0.281, 0.0, 0.0},\\n{111.56, 99.4, 2.0, 0.658, 3.426, 1.189, 4.0, 0.0},\\n{77.82, 99.365, 4.0, 3.048, 2.813, 0.791, 4.0, 0.0},\\n{29.46, 53.683, 1.0, 3.028, 2.225, 0.548, 0.0, 0.0},\\n{0.0, 0.0, 0.0, 3.267, 2.318, 0.963, 0.0, 0.0},\\n{92.35, 174.455, 1.0, 3.84, 3.373, 0.802, 1.0, 1.0},\\n{41.57, 57.28, 1.0, 1.838, 2.611, 1.175, 2.0, 0.0},\\n};'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_array_string_2d(x_test_np, 'x_test0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write TFLite model to a C source (or header) file\n",
    "with open(\"QSAR_x_test0_data_new\" + '.h', 'w') as file:\n",
    "    file.write(generate_array_string_2d(x_test_np, 'x_test0'))\n",
    "\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open(\"QSAR_x_train0_data_new\" + '.h', 'w') as file:\n",
    "    file.write(generate_array_string_2d(x_train_np, 'x_train0'))\n",
    "\n",
    "with open(\"QSAR_y_test0_data_new\" + '.h', 'w') as file:\n",
    "    file.write(generate_array_string_1d(y_test_np, 'y_test0'))\n",
    "\n",
    "with open(\"QSAR_y_train0_data_new\" + '.h', 'w') as file:\n",
    "    file.write(generate_array_string_1d(np.array(y_train), 'y_train0'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
